---
title: "APPLIED STATISTICS"
author: " "
date: "2023-12-04"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#                              GROUP MEMBERS:

                              "*SWETHA YANAMANDHALLA (811294573)*" 

                           "*SAI LAXMI PRIYANKA GANNAVARAPU (811283553)*" 

                              "*AJAYCHARY KANDUKURI (811294510)*"
                              
                              
                              
#   CONTRIBUTIONS(GRADE :0-5)
                              
  #SAI LAXMI PRIYANKA GANNAVARAPU (811283553) : 5
  
  #AJAYCHARY KANDUKURI (811294510):5

#1)
```{r}
library(alr4)
library(dplyr)
data_frame = Downer[ c("calving", "daysrec", "ck", "ast", "urea", "pcv")]
data_frame = na.omit(df)
data_frame

library(glmnet)   
set.seed(123)
train_index = sample(seq_len(nrow(Downer)), 0.8 * nrow(Downer))
train_data = Downer[train_index, ]
test_data = Downer[-train_index, ]

logistic_model = glm(outcome ~ calving + daysrec + ck + ast + urea + pcv,
                      data = train_data,
                      family = binomial)

predictions = predict(logistic_model, newdata = test_data, type = "response")
threshold = 0.5
predicted_classes = ifelse(predictions > threshold, 1, 0)
confusion_matrix = table(test_data$outcome, predicted_classes)
confusion_matrix
accuracy = sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy
```

#2)
```{r}
library(MASS)
data(Boston)
head(Boston)
library(glmnet)
set.seed(123)
predictors = colnames(Boston)[colnames(Boston) != "medv"]
x = scale(as.matrix(Boston[, predictors]))
y = Boston$medv
lasso_model = cv.glmnet(x, y, alpha = 1)
plot(lasso_model)
optimal_lambda = lasso_model$lambda.min
optimal_lambda

final_model = glmnet(x, y, alpha = 1, lambda = optimal_lambda)
coef(final_model)
lasso_coefficients = coef(final_model)
selected_features = rownames(lasso_coefficients)[lasso_coefficients[, 1] != 0]

predictions = predict(final_model, newx = x, s = optimal_lambda)
mse = mean((predictions - y)^2)
mse
selected_coefficients = coef(final_model, s = optimal_lambda)
selected_coefficients
selected_features = rownames(selected_coefficients)[selected_coefficients[, 1] != 0]
feature_importance = lasso_coefficients[rownames(lasso_coefficients) %in% selected_features, ]
feature_importance
```
**INTERPRETATION: Selected features like low crime rates, proximity to the Charles River, and increased residential land positively impact predicted house prices in Boston suburbs, while factors such as higher nitric oxide levels and longer commutes contribute to lower predicted prices.**

#3)
```{r}
data(faithful)
faithful_data = faithful
X = faithful_data$waiting
y = faithful_data$eruptions
r_squared_values = vector("numeric", 4)
for (degree in 1:4) {
  poly_model = lm(y ~ poly(X, degree, raw = TRUE))
  folds = cut(seq(1, length(y)), breaks = 10, labels = FALSE)
  cv_r_squared = sapply(1:10, function(i) {
    val_index = which(folds == i, arr.ind = TRUE)
    val_data = data.frame(X = X[val_index], y = y[val_index])
    train_data = data.frame(X = X[-val_index], y = y[-val_index])
    model = lm(y ~ poly(X, degree, raw = TRUE), data = train_data)
    predictions = predict(model, newdata = val_data)
    1 - sum((val_data$y - predictions)^2) / sum((val_data$y - mean(val_data$y))^2)
  })
  avg_r_squared = mean(cv_r_squared)
  r_squared_values[degree] = avg_r_squared
}
best_degree = which.max(r_squared_values)
best_avg_r_squared = max(r_squared_values)
best_degree
best_avg_r_squared
```