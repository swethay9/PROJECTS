---
title: "FINAL PROJECT"
subtitle: "MATH 40024/50024: Computational Statistics"
date: October 19, 2023
output: pdf_document

fontfamily: mathpazo
fontsize: 11pt
header-includes:
   - \linespread{1.05}
urlcolor: blue
---

__ACADEMIC INTEGRITY: Every student should complete the project by their own. A project report having high degree of similarity with work by any other student, or with any other document (e.g., found online) is considered plagiarism, and will not be accepted. The minimal consequence is that the student will receive the project score of 0, and the best possible overall course grade will be D. Additional consequences are described at http://www.kent.edu/policyreg/administrative-policy-regarding-student-cheating-and-plagiarism and will be strictly enforced.__

## Instruction

__Goal:__ The goal of the project is to go through the complete data analysis workflow to answer questions about your chosen topic using a real-life dataset. You will need to acquire the data, munge and explore the data, perform statistical analysis, and communicate the results.

__Report:__ Use this Rmd file as a template. Edit the file by adding your project title in the YAML, and including necessary information in the four sections: (1) Introduction, (2) Computational Methods, (3) Data Analysis and Results, and (4) Conclusion. 

__Submission:__ Please submit your project report as a PDF file (8-10 pages, flexible) to Canvas by __11:59 p.m. on December 10__. The PDF file should be generated by "knitting" the Rmd file. You may choose to first generate an HTML file (by changing the output format in the YAML to `output: html_document`) and then convert it to PDF. __20 points will be deducted if the submitted files are in wrong format.__ 

__Grade:__ The project will be graded based on your ability to (1) recognize and define research questions suitable for data-driven, computational approaches, (2) use computational methods to analyze data, (3) appropriately document the process (with R code) and clearly present the results, and (4) draw valid conclusions supported by the data analysis.

__Example topics:__

* [Post-Hurricane Vital Statistics](https://www.biorxiv.org/content/10.1101/407874v2)
* [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday)

__Datasets:__ I suggest to work on a dataset with at least thousands of observations and dozens of variables. You may consider (but are not restricted) to use the following data repositories: [Data.gov](https://catalog.data.gov/dataset), [Kaggle](https://www.kaggle.com/datasets), [FiveThirtyEight](https://data.fivethirtyeight.com/), [ProPublica](https://www.propublica.org/datastore/datasets), and [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)

\pagebreak

## Introduction [15 points]

* #  What research question(s) would you like to answer?

1)What have been the main sales trends over the years? Are there any specific products, categories, or geographical areas where sales are significantly increasing or decreasing?

2)Is it possible to spot trends in consumer behaviour, such as recurrent purchases or brand loyalty?

3)What is the impact of discounts on overall sales and profitability? Are certain products or customer segments more responsive to discounts?

* #  Why a data-driven, computational approach may be useful to answer the questions?

Because of the complexity of the Global Superstore Dataset many variables and large amounts of data a data-driven, computational method is required to extract subtle insights. The size and complexity of the large dataset make manual examination impractical. Statistical techniques and machine learning algorithms in particular are computationally intensive and are essential for detecting hidden patterns, identifying complex relationships, and performing predictive modelling. A thorough investigation of sales trends, consumer behaviour, and profitability dynamics is ensured by the computational power required to navigate and process the data effectively when multiple variables are integrated. Automation makes reproducibility easier, and scalability makes it possible to adjust to big datasets. The computational method offers a comprehensive understanding of the complex interactions within the Global Superstore Dataset by facilitating the synthesis of multifaceted information and speeding up the analysis.

* #  Describe the dataset that you choose.
```{r}
# choosen dataset
library(readr)
superstore <- read_csv("C:/Users/Sreedhar Jhansy/Desktop/superstore.csv")
View(superstore)

```


category: The category of products sold in the superstore.
city: The city where the order was placed.
country: The country in which the superstore is located.
customer_id: A unique identifier for each customer.
customer_name: The name of the customer who placed the order.
discount: The discount applied to the order.
market: The market or region where the superstore operates.

order_date: The date when the order was placed.
order_id: A unique identifier for each order.
order_priority: The priority level of the order.
product_id: A unique identifier for each product.
product_name: The name of the product.
profit: The profit generated from the order.
quantity: The quantity of products ordered.
region: The region where the order was placed.
row_id: A unique identifier for each row in the dataset.
sales: The total sales amount for the order.
segment: The customer segment (e.g., consumer, corporate, or home office).
ship_date: The date when the order was shipped.
ship_mode: The shipping mode used for the order.
shipping_cost: The cost of shipping for the order.
state: The state or region within the country.
sub_category: The sub-category of products within the main category.
year: The year in which the order was placed.
market2: Another column related to market information.
weeknum: The week number when the order was placed.

enteries:51290
column:26
 This dataset provides a comprehensive snapshot for diverse data analysis tasks in the context of the global superstore's operations.

## Computational Methods [30 points]

* # For the choosen dataset, what are the necessary data wrangling steps to make the data ready for subsequent analyses?

The steps for datawrangling  to the dataset is to load the dataset identify  the missing values and remove the missing values,identify and remove the duplicate values,convert the datatypes if needed.
```{r}
#identify missing values
missing_values = colSums(is.na(superstore))
#printing the missing values
missing_values


#removing the missing values
superstore = na.omit(superstore)
superstore
summary(superstore)

#identify duplicate rows
duplicate_rows = superstore[duplicated(superstore), ]
duplicate_rows
#removing duplicate rows
superstore = unique(superstore)
superstore




```

* # What exploratory analyses and modeling techniques can be used to answer the research questions?
1)Data Visualisation: To identify particular products, categories, or geographic areas with notable sales variations,create visualisations using tools like line charts, bar graphs, and heatmaps.


2)Customer Segmentation: To identify distinct consumer segments with distinctive purchasing patterns, apply clustering techniques.By identifying correlations between products that are frequently bought together, association rule mining can provide valuable insights into consumer preferences and brand loyalty.

3)Customer Segmentation: Using segmentation based on demographics or past purchases, examine whether specific customer segments react better to discounts.
Profitability Modelling: Using factors like product category and customer segments, develop predictive models to estimate how discounts affect overall profitability.

* #  What metrics will be used to evaluate the quality of the data analysis?

1)To identify particular regions of notable growth or decline, analyse the sales distribution across various products, categories, or geographic locations.
Metric: Sales Distribution by Product/Category/Geographical Area

2)explanation: This metric measures the proportion of customers who, over a given time period, make repeat purchases, indicating brand loyalty and recurring consumer behaviour.
Metric: Customer Retention Rate 

3)To determine how discounts affect financial metrics, consider the percentage change in overall sales and profitability after applying discounts.
Metric: Sales and Profitability Change as a Percentage



## Data Analysis and Results [40 points]

* #  Perform data analysis, document the analysis procedure, and evaluate the outcomes.
```{r}
#build the histogram
data = as.data.frame(sapply(superstore, as.numeric))
hist(data$Sales)
hist(data$Year)

library(ggplot2)  
ggplot(superstore, aes(x = superstore$Category, fill = Sales)) +
  geom_bar(position = "dodge", stat = "count", color = "black") +
  labs(x = "City", y = "sales") +
  theme_minimal()
```



```{r}
#finding the correlations
numerical_columns = sapply(superstore, is.numeric)
numerical_data = superstore[, numerical_columns]
correlation = cor(numerical_data)

#finding the chi square
contingency_table1 = table(superstore$Category, superstore$Sales)
chi_square1 = chisq.test(contingency_table1)

contingency_table2 = table(superstore$Sales, superstore$City)
chi_square2 = chisq.test(contingency_table2)

contingency_table3 = table(superstore$Customer.Name, superstore$Sales)
chi_square3 = chisq.test(contingency_table3)

contingency_table4 = table(superstore$Country, superstore$Sales)
chi_square4 = chisq.test(contingency_table4)

contingency_table5 = table(superstore$Category, superstore$Sales)
chi_square5 = chisq.test(contingency_table5)

```
```{r}

#linearmodel
superstore$Sales = as.factor(superstore$Sales)
str(superstore)

superstore_subset = superstore[sample(nrow(superstore), 10000), ]

# Fit the model on the subset


index=sample(2,nrow(superstore_subset),replace = TRUE,prob =c(.80,.20))
train_set = superstore_subset[index == 1,]
test_set = superstore_subset[index== 2, ]



train_set
test_set
library(ROCR)
library(caTools)
```

```{r}
# Fit logistic regression model
log_reg1 = glm(Sales ~ Year,  data = train_set, family = "binomial")


summary(log_reg1)
plot(log_reg1, col = "black")



# Make predictions on the test set
predict_reg = predict(log_reg1, newdata = test_set, type = "response")

# Threshold for classification
threshold = 0.5

# Convert predicted probabilities to factor with levels 0 and 1
predict_reg = factor(ifelse(predict_reg > threshold, "1", "0"), levels = c("0", "1"))

# Ensure that test_set$Sales is a factor with the same levels
test_set$Sales= factor(test_set$Sales, levels = levels(predict_reg))

# Compare predicted values with actual values
comparison = test_set$Sales == predict_reg

# Ensure that comparison is a logical vector
comparison = as.logical(comparison)

# Create a confusion matrix
conf_matrix = caret::confusionMatrix(data = predict_reg, reference = test_set$Sales)
conf_matrix




```
* #  Present the data analysis results.
```{r}
correlation
chi_square1
chi_square2
chi_square3
chi_square4
chi_square5



```

 



* #  Interpret the results in a way to address the research questions.

The histograms give an early look at the sales distributions and the evolution of time, giving a general idea of their patterns. However, the logistic regression model becomes an essential tool to delve deeper into the dynamics of sales trends, particularly with regard to particular products, categories, and geographic areas. We can understand the complex relationship between time and sales and determine whether sales are trending upward or downward by fitting the model with the 'Year' variable. The model summary, which includes the 'Year' coefficient and its importance, gives us the ability to decipher the direction and strength of this time-related impact on sales.The resulting confusion matrix provides useful metrics like precision, recall, and F1 score and acts as a metre for evaluating how well the model classifies sales. It is generated from predictions on the test set. 




## Conclusion [15 points]

* #  Does the analysis answer the research questions?
yes , it gave me the answers.
The first research question is partially addressed by the time series analysis in R that is provided. It uses a moving average to smooth out trends and shows overall sales trends over time. Nevertheless, because it lacks particular analyses for identifying trends in products, categories, and geographic areas, it is unable to provide a comprehensive response to the question. Further analyses are needed to address the second research question about consumer behavior, which relates to recurring purchases or brand loyalty, and the third question about the effect of discounts on sales and profitability. A thorough understanding of the dataset and effective answers to all research questions require specific segmentation based on customer behavior, brand loyalty assessments, and in-depth discount impact analyses on products and customer segments.




* #  Discuss the scope and generalizability of the analysis.

The scope of the analysis is limited to visualizing overall sales trends over the years and applying a moving average for trend smoothing. It provides a broad understanding of general sales patterns but lacks specificity in exploring trends related to specific products, categories, or geographical areas. The generalizability of the analysis is constrained by its focus on the provided dataset, and the insights derived may not be readily applicable to broader contexts. To enhance the analysis's scope and generalizability, more granular analyses, such as product-level trends and regional variations, along with the incorporation of advanced statistical or machine learning techniques, would be necessary to capture a more comprehensive and transferable understanding of sales dynamics.



* # Discuss potential limitations and possibilities for improvement.

There are various potential limitations to the analysis that should be taken into account. First of all, its granularity is limited as it primarily concentrates on overall sales trends without exploring particular product categories or geographic areas. Furthermore, the analysis may not be able to identify intricate patterns or generate predictions in the absence of statistical models or machine learning methods. Moving averages can be helpful for smoothing trends, but they can also oversimplify dynamics and miss small changes. Additionally, without addressing possible problems with data quality or outliers, the analysis makes the assumption that the dataset is impartial and representative. More sophisticated statistical techniques, in-depth product and regional analyses, and the investigation of predictive modelling could all be used to enhance the analysis. 

