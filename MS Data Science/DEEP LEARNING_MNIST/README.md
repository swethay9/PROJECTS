# 🧠 Foundations of Deep Learning: MNIST Digit Recognition  
# AUTHOR
Swetha Yanamandhalla

Date:Dec 13,2024


 ## Enhanced neural network architecture achieving 85.3% accuracy on MNIST! 

---

## Project Overview  
This repository contains **two Jupyter notebooks** designed to implement, analyze, and compare different deep learning architectures for digit classification on the **MNIST dataset**. The goal is to evaluate improvements in performance when extending a baseline neural network.  

## Notebooks Included:  
## extended_network.ipynb – An optimized architecture with additional layers and modifications.  
## original_network.ipynb – A benchmark implementation of the original network.  

---

## Notebook Descriptions  

### extended_network.ipynb  
🔸 Implements an **extended version** of the original neural network.  
🔸 **12 code cells** covering architecture design, training, and evaluation.  
🔸 Introduces enhancements to improve accuracy and model generalization.  

### 🔹 `original_network.ipynb`  
🔸 Implements the **baseline neural network** for comparison.  
🔸 **12 code cells** focused on defining, training, and evaluating the network.  
🔸 Serves as a reference model to measure improvements made in the extended version.  

---

## How to Use 

1️⃣ Ensure **Python 3.8+** is installed on your system.  
2️⃣ Install the necessary dependencies:  
   ```bash
   pip install tensorflow torch numpy matplotlib jupyter
   ```  
3️⃣ Open Jupyter Notebook:  
   ```bash
   jupyter notebook
   ```  
4️⃣ Run the notebooks in order to **train and evaluate** the models.  

---

## Project Structure  

 ## extended_network.ipynb – Advanced neural network with improved architecture.  
 ## original_network.ipynb – Standard baseline model for comparison.  

---

## Purpose  
The aim of this project is to **analyze the impact of architectural changes** in neural networks by comparing the **extended model against the original version**. By running the notebooks, users can:  
✔️ Observe improvements in model accuracy.  
✔️ Understand the role of additional layers and optimizations.  
✔️ Evaluate trade-offs in computation and performance.  

---

## Results  
🔹 The **extended network achieved 85.3% accuracy**, demonstrating significant improvement over the original architecture.  
🔹 Detailed **performance metrics** and visualizations are available in the output sections of each notebook.  

---

## License  
This project is **open-source** under the **MIT License** – feel free to use, modify, and build upon it!  

---
## Contact
For any inquiries or suggestions, feel free to reach out:
- **Name**: Swetha
- **Email**: swethachowdhary33@gmail.com
- **GitHub**: [SWETHAY9](https://github.com/swethay9)


